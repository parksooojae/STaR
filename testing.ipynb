{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc72b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/Desktop/paper-rep/learning-STaR/base/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# load the commonsense_qa trian set and convert to pandas\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "train = pd.DataFrame(dataset[\"train\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc99b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = len(train)\n",
    "\n",
    "# # randomly choose 10 indices from the training set to use for few-shot prompting\n",
    "# # # these 10 indices will stay the same throughout the whole STaR loop\n",
    "# # random.seed(67)\n",
    "# # random_indices = random.sample(range(train_size), 10)\n",
    "# # exclude the randomly selected indices from the training set\n",
    "\n",
    "# random_indices = [1225, 1902, 6690, 7661, 6804, 4372, 7036, 7991, 4555, 3338]\n",
    "\n",
    "# print(random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecff60a",
   "metadata": {},
   "source": [
    "### Build fewshot prompt example set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a853f4",
   "metadata": {},
   "source": [
    "see fewshot_prompt.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b225dc7",
   "metadata": {},
   "source": [
    "### Test building prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320b8d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Example 1\n",
      "Q: Sarah felt bad about their odds. Billy disagreed with her. What did he think about their odds?\n",
      "Answer Choices:\n",
      "(a) first rate\n",
      "(b) adequate\n",
      "(c) good\n",
      "(d) propitious\n",
      "(e) sufficient\n",
      "A: The answer must be the opposite of Sarah's view since Billy disagreed. Sarah \"felt bad,\" so Billy must think positively. 'Adequate' and 'sufficient' mean barely acceptable, not the opposite of bad. 'First rate' typically describes quality rather than odds. 'Good' is positive but generic. Only option D is a term specifically used to describe fortunate circumstances. Therefore, the answer is propitious (d).\n",
      "\n",
      "### Example 2\n",
      "Q: Where is an expressway unlikely to be found?\n",
      "Answer Choices:\n",
      "(a) ocean\n",
      "(b) map\n",
      "(c) canada\n",
      "(d) large city\n",
      "(e) country\n",
      "A: The answer must be a place where expressways typically don't exist. A map can depict an expressway. Canada and large cities have expressways. An ocean cannot have roads at all, but the question asks \"unlikely\" not \"impossible.\" Rural areas (the \"country\") lack the population density that justifies expressway construction. Therefore, the answer is country (e).\n",
      "\n",
      "### Example 3\n",
      "Q: He was trying to eat more green, as in simple ingredients rather than what foods?\n",
      "Answer Choices:\n",
      "(a) ripe\n",
      "(b) greenweed\n",
      "(c) yellow\n",
      "(d) processed\n",
      "(e) cloy\n",
      "A: The answer must contrast with \"green\" and \"simple ingredients.\" 'Ripe' and 'yellow' describe natural food states. 'Greenweed' is a plant. 'Cloy' means to sicken with excess. Only option D represents a category of food that is the opposite of simple, natural ingredients. Therefore, the answer is processed (d).\n",
      "\n",
      "### Example 4\n",
      "Q: What is the opposite of what will happen if you're lying constantly?\n",
      "Answer Choices:\n",
      "(a) feeling guilty\n",
      "(b) confusion\n",
      "(c) unhappiness\n",
      "(d) good things\n",
      "(e) rewards\n",
      "A: The answer must be the opposite of consequences from constant lying. Lying leads to negative outcomes like guilt, confusion, and unhappinessâ€”so these are NOT the opposite. 'Rewards' implies earning something specific. Only option D represents the general opposite of negative consequences. Therefore, the answer is good things (d).\n",
      "\n",
      "### Example 5\n",
      "Q: Where could an electric razor be exposed to water?\n",
      "Answer Choices:\n",
      "(a) medicine cabinet\n",
      "(b) outside\n",
      "(c) k mart\n",
      "(d) bathroom\n",
      "(e) wal mart\n",
      "A: The answer must be a location where water exposure is likely. A medicine cabinet is dry storage. K Mart and Walmart are retail stores. 'Outside' has weather but isn't a typical razor location. Only option D is where razors are commonly used near sinks and showers. Therefore, the answer is bathroom (d).\n",
      "\n",
      "### Example 6\n",
      "Q: What are you doing when you're climbing a mountain?\n",
      "Answer Choices:\n",
      "(a) falling down\n",
      "(b) getting higher\n",
      "(c) vertigo\n",
      "(d) elevation\n",
      "(e) accomplishment\n",
      "A: The answer must describe an ongoing action during climbing. 'Falling down' is the opposite of climbing. 'Vertigo' is a sensation, not an action. 'Elevation' and 'accomplishment' are nouns, not actions. Only option B describes what physically occurs as you ascend. Therefore, the answer is getting higher (b).\n",
      "\n",
      "### Example 7\n",
      "Q: The person was cold and wet, where should he go?\n",
      "Answer Choices:\n",
      "(a) building\n",
      "(b) back home\n",
      "(c) hospital\n",
      "(d) demonstration\n",
      "(e) bus stop\n",
      "A: The answer must provide relief from cold and wet conditions. A demonstration and bus stop are outdoors. A hospital is for medical emergencies. 'Back home' assumes home is accessible. Only option A provides immediate indoor shelter regardless of where you are. Therefore, the answer is building (a).\n",
      "\n",
      "### Example 8\n",
      "Q: What might a gentleman own and go inside of?\n",
      "Answer Choices:\n",
      "(a) garage\n",
      "(b) big house\n",
      "(c) movie\n",
      "(d) restaurant\n",
      "(e) tuxedo\n",
      "A: The answer must be something a gentleman can both own and physically enter. A garage is auxiliary storage. A movie is entertainment, not a structure. A restaurant is a commercial business. A tuxedo is worn, not entered. Only option B is a residence that can be owned and inhabited. Therefore, the answer is big house (b).\n",
      "\n",
      "### Example 9\n",
      "Q: What could you be doing if you are relaxing and are awake but have your eyes closed?\n",
      "Answer Choices:\n",
      "(a) read book\n",
      "(b) listening to music\n",
      "(c) falling asleep\n",
      "(d) watching tv\n",
      "(e) reading\n",
      "A: The answer must be an activity done while relaxed, awake, and with eyes closed. Reading a book, reading, and watching TV all require open eyes. Falling asleep contradicts being awake. Only option B can be enjoyed while relaxed with eyes closed and still conscious. Therefore, the answer is listening to music (b).\n",
      "\n",
      "### Example 10\n",
      "Q: If a cottage is above the cloud line, where is it?\n",
      "Answer Choices:\n",
      "(a) village\n",
      "(b) rural area\n",
      "(c) mountains\n",
      "(d) fairy story\n",
      "(e) valley\n",
      "A: The answer must be a location where structures can exist above the cloud line. Villages, rural areas, and valleys are typically at lower elevations. A fairy story is fictional, not a physical location. Only option C reaches elevations above where clouds typically form. Therefore, the answer is mountains (c).\n",
      "\n",
      "### Now answer this\n",
      "Q: If your hair gel stinks it is too what?\n",
      "Answer Choices: (a) barbers\n",
      "(b) messy\n",
      "(c) beauty salon\n",
      "(d) perfumery\n",
      "(e) bathroom\n",
      "Output:\n",
      "{\n",
      "  \"rationale\": \"The answer must be [constraint from question]. [Reasoning connecting constraint to answer]. Therefore, the answer is [answer] ([letter])\",\n",
      "  \"answer\": \"<capital letter corresponding to answer choices>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample one question from the train dataset\n",
    "\n",
    "# Read the few-shot prompt from the file\n",
    "with open('data/prompts/fewshot_prompt.txt', 'r') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "\n",
    "\n",
    "random_idx = random.randint(0, len(train) - 1)\n",
    "sample_question = train.iloc[random_idx]\n",
    "\n",
    "# Extract question and format answer choices\n",
    "question_text = sample_question['question']\n",
    "choices_text = sample_question['choices']['text']\n",
    "\n",
    "# Format answer choices as (a) choice1 (b) choice2 etc.\n",
    "formatted_choices = \"\"\n",
    "for i, choice in enumerate(choices_text):\n",
    "    letter = chr(ord('a') + i)  # Convert 0,1,2,3,4 to a,b,c,d,e\n",
    "    formatted_choices += f\"({letter}) {choice}\\n\"\n",
    "\n",
    "# Remove the trailing newline\n",
    "formatted_choices = formatted_choices.strip()\n",
    "\n",
    "# Replace placeholders in the prompt\n",
    "prompt = prompt.replace(\"{{question}}\", question_text)\n",
    "test_prompt = prompt.replace(\"{{answer_choices}}\", formatted_choices)\n",
    "\n",
    "\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33b8ab",
   "metadata": {},
   "source": [
    "### import model and test prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f7c5d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1a227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
